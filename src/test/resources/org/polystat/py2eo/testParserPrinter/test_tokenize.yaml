python: |
  from test import support
  from tokenize import (tokenize, _tokenize, untokenize, NUMBER, NAME, OP,
                       STRING, ENDMARKER, ENCODING, tok_name, detect_encoding,
                       open as tokenize_open, Untokenizer, generate_tokens,
                       NEWLINE)
  from io import BytesIO, StringIO
  import unittest
  from unittest import TestCase, mock
  from test.test_grammar import (VALID_UNDERSCORE_LITERALS,
                                 INVALID_UNDERSCORE_LITERALS)
  import os
  import token
  
  
  # Converts a source string into a list of textual representation
  # of the tokens such as:
  # `    NAME       'if'          (1, 0) (1, 2)`
  # to make writing tests easier.
  def stringify_tokens_from_source(token_generator, source_string):
      result = []
      num_lines = len(source_string.splitlines())
      missing_trailing_nl = source_string[-1] not in '\r\n'
  
      for type, token, start, end, line in token_generator:
          if type == ENDMARKER:
              break
          # Ignore the new line on the last line if the input lacks one
          if missing_trailing_nl and type == NEWLINE and end[0] == num_lines:
              continue
          type = tok_name[type]
          result.append(f"    {type:10} {token!r:13} {start} {end}")
  
      return result
  
  class TokenizeTest(TestCase):
      # Tests for the tokenize module.
  
      # The tests can be really simple. Given a small fragment of source
      # code, print out a table with tokens. The ENDMARKER, ENCODING and
      # final NEWLINE are omitted for brevity.
  
      def check_tokenize(self, s, expected):
          # Format the tokens in s in a table format.
          # The ENDMARKER and final NEWLINE are omitted.
          f = BytesIO(s.encode('utf-8'))
          result = stringify_tokens_from_source(tokenize(f.readline), s)
  
          self.assertEqual(result,
                           ["    ENCODING   'utf-8'       (0, 0) (0, 0)"] +
                           expected.rstrip().splitlines())
  
      def test_implicit_newline(self):
          # Make sure that the tokenizer puts in an implicit NEWLINE
          # when the input lacks a trailing new line.
          f = BytesIO("x".encode('utf-8'))
          tokens = list(tokenize(f.readline))
          self.assertEqual(tokens[-2].type, NEWLINE)
          self.assertEqual(tokens[-1].type, ENDMARKER)
  
      def test_basic(self):
          self.check_tokenize("1 + 1", """\
      NUMBER     '1'           (1, 0) (1, 1)
      OP         '+'           (1, 2) (1, 3)
      NUMBER     '1'           (1, 4) (1, 5)
      """)
          self.check_tokenize("if False:\n"
                              "    # NL\n"
                              "    \n"
                              "    True = False # NEWLINE\n", """\
      NAME       'if'          (1, 0) (1, 2)
      NAME       'False'       (1, 3) (1, 8)
      OP         ':'           (1, 8) (1, 9)
      NEWLINE    '\\n'          (1, 9) (1, 10)
      COMMENT    '# NL'        (2, 4) (2, 8)
      NL         '\\n'          (2, 8) (2, 9)
      NL         '\\n'          (3, 4) (3, 5)
      INDENT     '    '        (4, 0) (4, 4)
      NAME       'True'        (4, 4) (4, 8)
      OP         '='           (4, 9) (4, 10)
      NAME       'False'       (4, 11) (4, 16)
      COMMENT    '# NEWLINE'   (4, 17) (4, 26)
      NEWLINE    '\\n'          (4, 26) (4, 27)
      DEDENT     ''            (5, 0) (5, 0)
      """)
          indent_error_file = b"""\
  def k(x):
      x += 2
    x += 5
  """
          readline = BytesIO(indent_error_file).readline
          with self.assertRaisesRegex(IndentationError,
                                      "unindent does not match any "
                                      "outer indentation level"):
              for tok in tokenize(readline):
                  pass
  
      def test_int(self):
          # Ordinary integers and binary operators
          self.check_tokenize("0xff <= 255", """\
      NUMBER     '0xff'        (1, 0) (1, 4)
      OP         '<='          (1, 5) (1, 7)
      NUMBER     '255'         (1, 8) (1, 11)
      """)
          self.check_tokenize("0b10 <= 255", """\
      NUMBER     '0b10'        (1, 0) (1, 4)
      OP         '<='          (1, 5) (1, 7)
      NUMBER     '255'         (1, 8) (1, 11)
      """)
          self.check_tokenize("0o123 <= 0O123", """\
      NUMBER     '0o123'       (1, 0) (1, 5)
      OP         '<='          (1, 6) (1, 8)
      NUMBER     '0O123'       (1, 9) (1, 14)
      """)
          self.check_tokenize("1234567 > ~0x15", """\
      NUMBER     '1234567'     (1, 0) (1, 7)
      OP         '>'           (1, 8) (1, 9)
      OP         '~'           (1, 10) (1, 11)
      NUMBER     '0x15'        (1, 11) (1, 15)
      """)
          self.check_tokenize("2134568 != 1231515", """\
      NUMBER     '2134568'     (1, 0) (1, 7)
      OP         '!='          (1, 8) (1, 10)
      NUMBER     '1231515'     (1, 11) (1, 18)
      """)
          self.check_tokenize("(-124561-1) & 200000000", """\
      OP         '('           (1, 0) (1, 1)
      OP         '-'           (1, 1) (1, 2)
      NUMBER     '124561'      (1, 2) (1, 8)
      OP         '-'           (1, 8) (1, 9)
      NUMBER     '1'           (1, 9) (1, 10)
      OP         ')'           (1, 10) (1, 11)
      OP         '&'           (1, 12) (1, 13)
      NUMBER     '200000000'   (1, 14) (1, 23)
      """)
          self.check_tokenize("0xdeadbeef != -1", """\
      NUMBER     '0xdeadbeef'  (1, 0) (1, 10)
      OP         '!='          (1, 11) (1, 13)
      OP         '-'           (1, 14) (1, 15)
      NUMBER     '1'           (1, 15) (1, 16)
      """)
          self.check_tokenize("0xdeadc0de & 12345", """\
      NUMBER     '0xdeadc0de'  (1, 0) (1, 10)
      OP         '&'           (1, 11) (1, 12)
      NUMBER     '12345'       (1, 13) (1, 18)
      """)
          self.check_tokenize("0xFF & 0x15 | 1234", """\
      NUMBER     '0xFF'        (1, 0) (1, 4)
      OP         '&'           (1, 5) (1, 6)
      NUMBER     '0x15'        (1, 7) (1, 11)
      OP         '|'           (1, 12) (1, 13)
      NUMBER     '1234'        (1, 14) (1, 18)
      """)
  
      def test_long(self):
          # Long integers
          self.check_tokenize("x = 0", """\
      NAME       'x'           (1, 0) (1, 1)
      OP         '='           (1, 2) (1, 3)
      NUMBER     '0'           (1, 4) (1, 5)
      """)
          self.check_tokenize("x = 0xfffffffffff", """\
      NAME       'x'           (1, 0) (1, 1)
      OP         '='           (1, 2) (1, 3)
      NUMBER     '0xfffffffffff' (1, 4) (1, 17)
      """)
          self.check_tokenize("x = 123141242151251616110", """\
      NAME       'x'           (1, 0) (1, 1)
      OP         '='           (1, 2) (1, 3)
      NUMBER     '123141242151251616110' (1, 4) (1, 25)
      """)
          self.check_tokenize("x = -15921590215012591", """\
      NAME       'x'           (1, 0) (1, 1)
      OP         '='           (1, 2) (1, 3)
      OP         '-'           (1, 4) (1, 5)
      NUMBER     '15921590215012591' (1, 5) (1, 22)
      """)
  
      def test_float(self):
          # Floating point numbers
          self.check_tokenize("x = 3.14159", """\
      NAME       'x'           (1, 0) (1, 1)
      OP         '='           (1, 2) (1, 3)
      NUMBER     '3.14159'     (1, 4) (1, 11)
      """)
          self.check_tokenize("x = 314159.", """\
      NAME       'x'           (1, 0) (1, 1)
      OP         '='           (1, 2) (1, 3)
      NUMBER     '314159.'     (1, 4) (1, 11)
      """)
          self.check_tokenize("x = .314159", """\
      NAME       'x'           (1, 0) (1, 1)
      OP         '='           (1, 2) (1, 3)
      NUMBER     '.314159'     (1, 4) (1, 11)
      """)
          self.check_tokenize("x = 3e14159", """\
      NAME       'x'           (1, 0) (1, 1)
      OP         '='           (1, 2) (1, 3)
      NUMBER     '3e14159'     (1, 4) (1, 11)
      """)
          self.check_tokenize("x = 3E123", """\
      NAME       'x'           (1, 0) (1, 1)
      OP         '='           (1, 2) (1, 3)
      NUMBER     '3E123'       (1, 4) (1, 9)
      """)
          self.check_tokenize("x+y = 3e-1230", """\
      NAME       'x'           (1, 0) (1, 1)
      OP         '+'           (1, 1) (1, 2)
      NAME       'y'           (1, 2) (1, 3)
      OP         '='           (1, 4) (1, 5)
      NUMBER     '3e-1230'     (1, 6) (1, 13)
      """)
          self.check_tokenize("x = 3.14e159", """\
      NAME       'x'           (1, 0) (1, 1)
      OP         '='           (1, 2) (1, 3)
      NUMBER     '3.14e159'    (1, 4) (1, 12)
      """)
  
      def test_underscore_literals(self):
          def number_token(s):
              f = BytesIO(s.encode('utf-8'))
              for toktype, token, start, end, line in tokenize(f.readline):
                  if toktype == NUMBER:
                      return token
              return 'invalid token'
          for lit in VALID_UNDERSCORE_LITERALS:
              if '(' in lit:
                  # this won't work with compound complex inputs
                  continue
              self.assertEqual(number_token(lit), lit)
          for lit in INVALID_UNDERSCORE_LITERALS:
              self.assertNotEqual(number_token(lit), lit)
  
      def test_string(self):
          # String literals
          self.check_tokenize("x = ''; y = \"\"", """\
      NAME       'x'           (1, 0) (1, 1)
      OP         '='           (1, 2) (1, 3)
      STRING     "''"          (1, 4) (1, 6)
      OP         ';'           (1, 6) (1, 7)
      NAME       'y'           (1, 8) (1, 9)
      OP         '='           (1, 10) (1, 11)
      STRING     '""'          (1, 12) (1, 14)
      """)
          self.check_tokenize("x = '\"'; y = \"'\"", """\
      NAME       'x'           (1, 0) (1, 1)
      OP         '='           (1, 2) (1, 3)
      STRING     '\\'"\\''       (1, 4) (1, 7)
      OP         ';'           (1, 7) (1, 8)
      NAME       'y'           (1, 9) (1, 10)
      OP         '='           (1, 11) (1, 12)
      STRING     '"\\'"'        (1, 13) (1, 16)
      """)
          self.check_tokenize("x = \"doesn't \"shrink\", does it\"", """\
      NAME       'x'           (1, 0) (1, 1)
      OP         '='           (1, 2) (1, 3)
      STRING     '"doesn\\'t "' (1, 4) (1, 14)
      NAME       'shrink'      (1, 14) (1, 20)
      STRING     '", does it"' (1, 20) (1, 31)
      """)
          self.check_tokenize("x = 'abc' + 'ABC'", """\
      NAME       'x'           (1, 0) (1, 1)
      OP         '='           (1, 2) (1, 3)
      STRING     "'abc'"       (1, 4) (1, 9)
      OP         '+'           (1, 10) (1, 11)
      STRING     "'ABC'"       (1, 12) (1, 17)
      """)
          self.check_tokenize('y = "ABC" + "ABC"', """\
      NAME       'y'           (1, 0) (1, 1)
      OP         '='           (1, 2) (1, 3)
      STRING     '"ABC"'       (1, 4) (1, 9)
      OP         '+'           (1, 10) (1, 11)
      STRING     '"ABC"'       (1, 12) (1, 17)
      """)
          self.check_tokenize("x = r'abc' + r'ABC' + R'ABC' + R'ABC'", """\
      NAME       'x'           (1, 0) (1, 1)
      OP         '='           (1, 2) (1, 3)
      STRING     "r'abc'"      (1, 4) (1, 10)
      OP         '+'           (1, 11) (1, 12)
      STRING     "r'ABC'"      (1, 13) (1, 19)
      OP         '+'           (1, 20) (1, 21)
      STRING     "R'ABC'"      (1, 22) (1, 28)
      OP         '+'           (1, 29) (1, 30)
      STRING     "R'ABC'"      (1, 31) (1, 37)
      """)
          self.check_tokenize('y = r"abc" + r"ABC" + R"ABC" + R"ABC"', """\
      NAME       'y'           (1, 0) (1, 1)
      OP         '='           (1, 2) (1, 3)
      STRING     'r"abc"'      (1, 4) (1, 10)
      OP         '+'           (1, 11) (1, 12)
      STRING     'r"ABC"'      (1, 13) (1, 19)
      OP         '+'           (1, 20) (1, 21)
      STRING     'R"ABC"'      (1, 22) (1, 28)
      OP         '+'           (1, 29) (1, 30)
      STRING     'R"ABC"'      (1, 31) (1, 37)
      """)
  
          self.check_tokenize("u'abc' + U'abc'", """\
      STRING     "u'abc'"      (1, 0) (1, 6)
      OP         '+'           (1, 7) (1, 8)
      STRING     "U'abc'"      (1, 9) (1, 15)
      """)
          self.check_tokenize('u"abc" + U"abc"', """\
      STRING     'u"abc"'      (1, 0) (1, 6)
      OP         '+'           (1, 7) (1, 8)
      STRING     'U"abc"'      (1, 9) (1, 15)
      """)
  
          self.check_tokenize("b'abc' + B'abc'", """\
      STRING     "b'abc'"      (1, 0) (1, 6)
      OP         '+'           (1, 7) (1, 8)
      STRING     "B'abc'"      (1, 9) (1, 15)
      """)
          self.check_tokenize('b"abc" + B"abc"', """\
      STRING     'b"abc"'      (1, 0) (1, 6)
      OP         '+'           (1, 7) (1, 8)
      STRING     'B"abc"'      (1, 9) (1, 15)
      """)
          self.check_tokenize("br'abc' + bR'abc' + Br'abc' + BR'abc'", """\
      STRING     "br'abc'"     (1, 0) (1, 7)
      OP         '+'           (1, 8) (1, 9)
      STRING     "bR'abc'"     (1, 10) (1, 17)
      OP         '+'           (1, 18) (1, 19)
      STRING     "Br'abc'"     (1, 20) (1, 27)
      OP         '+'           (1, 28) (1, 29)
      STRING     "BR'abc'"     (1, 30) (1, 37)
      """)
          self.check_tokenize('br"abc" + bR"abc" + Br"abc" + BR"abc"', """\
      STRING     'br"abc"'     (1, 0) (1, 7)
      OP         '+'           (1, 8) (1, 9)
      STRING     'bR"abc"'     (1, 10) (1, 17)
      OP         '+'           (1, 18) (1, 19)
      STRING     'Br"abc"'     (1, 20) (1, 27)
      OP         '+'           (1, 28) (1, 29)
      STRING     'BR"abc"'     (1, 30) (1, 37)
      """)
          self.check_tokenize("rb'abc' + rB'abc' + Rb'abc' + RB'abc'", """\
      STRING     "rb'abc'"     (1, 0) (1, 7)
      OP         '+'           (1, 8) (1, 9)
      STRING     "rB'abc'"     (1, 10) (1, 17)
      OP         '+'           (1, 18) (1, 19)
      STRING     "Rb'abc'"     (1, 20) (1, 27)
      OP         '+'           (1, 28) (1, 29)
      STRING     "RB'abc'"     (1, 30) (1, 37)
      """)
          self.check_tokenize('rb"abc" + rB"abc" + Rb"abc" + RB"abc"', """\
      STRING     'rb"abc"'     (1, 0) (1, 7)
      OP         '+'           (1, 8) (1, 9)
      STRING     'rB"abc"'     (1, 10) (1, 17)
      OP         '+'           (1, 18) (1, 19)
      STRING     'Rb"abc"'     (1, 20) (1, 27)
      OP         '+'           (1, 28) (1, 29)
      STRING     'RB"abc"'     (1, 30) (1, 37)
      """)
          # Check 0, 1, and 2 character string prefixes.
          self.check_tokenize(r'"a\
  de\
  fg"', """\
      STRING     '"a\\\\\\nde\\\\\\nfg"\' (1, 0) (3, 3)
      """)
          self.check_tokenize(r'u"a\
  de"', """\
      STRING     'u"a\\\\\\nde"\'  (1, 0) (2, 3)
      """)
          self.check_tokenize(r'rb"a\
  d"', """\
      STRING     'rb"a\\\\\\nd"\'  (1, 0) (2, 2)
      """)
          self.check_tokenize(r'"""a\
  b"""', """\
      STRING     '\"\""a\\\\\\nb\"\""' (1, 0) (2, 4)
      """)
          self.check_tokenize(r'u"""a\
  b"""', """\
      STRING     'u\"\""a\\\\\\nb\"\""' (1, 0) (2, 4)
      """)
          self.check_tokenize(r'rb"""a\
  b\
  c"""', """\
      STRING     'rb"\""a\\\\\\nb\\\\\\nc"\""' (1, 0) (3, 4)
      """)
          self.check_tokenize('f"abc"', """\
      STRING     'f"abc"'      (1, 0) (1, 6)
      """)
          self.check_tokenize('fR"a{b}c"', """\
      STRING     'fR"a{b}c"'   (1, 0) (1, 9)
      """)
          self.check_tokenize('f"""abc"""', """\
      STRING     'f\"\"\"abc\"\"\"'  (1, 0) (1, 10)
      """)
          self.check_tokenize(r'f"abc\
  def"', """\
      STRING     'f"abc\\\\\\ndef"' (1, 0) (2, 4)
      """)
          self.check_tokenize(r'Rf"abc\
  def"', """\
      STRING     'Rf"abc\\\\\\ndef"' (1, 0) (2, 4)
      """)
  
      def test_function(self):
          self.check_tokenize("def d22(a, b, c=2, d=2, *k): pass", """\
      NAME       'def'         (1, 0) (1, 3)
      NAME       'd22'         (1, 4) (1, 7)
      OP         '('           (1, 7) (1, 8)
      NAME       'a'           (1, 8) (1, 9)
      OP         ','           (1, 9) (1, 10)
      NAME       'b'           (1, 11) (1, 12)
      OP         ','           (1, 12) (1, 13)
      NAME       'c'           (1, 14) (1, 15)
      OP         '='           (1, 15) (1, 16)
      NUMBER     '2'           (1, 16) (1, 17)
      OP         ','           (1, 17) (1, 18)
      NAME       'd'           (1, 19) (1, 20)
      OP         '='           (1, 20) (1, 21)
      NUMBER     '2'           (1, 21) (1, 22)
      OP         ','           (1, 22) (1, 23)
      OP         '*'           (1, 24) (1, 25)
      NAME       'k'           (1, 25) (1, 26)
      OP         ')'           (1, 26) (1, 27)
      OP         ':'           (1, 27) (1, 28)
      NAME       'pass'        (1, 29) (1, 33)
      """)
          self.check_tokenize("def d01v_(a=1, *k, **w): pass", """\
      NAME       'def'         (1, 0) (1, 3)
      NAME       'd01v_'       (1, 4) (1, 9)
      OP         '('           (1, 9) (1, 10)
      NAME       'a'           (1, 10) (1, 11)
      OP         '='           (1, 11) (1, 12)
      NUMBER     '1'           (1, 12) (1, 13)
      OP         ','           (1, 13) (1, 14)
      OP         '*'           (1, 15) (1, 16)
      NAME       'k'           (1, 16) (1, 17)
      OP         ','           (1, 17) (1, 18)
      OP         '**'          (1, 19) (1, 21)
      NAME       'w'           (1, 21) (1, 22)
      OP         ')'           (1, 22) (1, 23)
      OP         ':'           (1, 23) (1, 24)
      NAME       'pass'        (1, 25) (1, 29)
      """)
          self.check_tokenize("def d23(a: str, b: int=3) -> int: pass", """\
      NAME       'def'         (1, 0) (1, 3)
      NAME       'd23'         (1, 4) (1, 7)
      OP         '('           (1, 7) (1, 8)
      NAME       'a'           (1, 8) (1, 9)
      OP         ':'           (1, 9) (1, 10)
      NAME       'str'         (1, 11) (1, 14)
      OP         ','           (1, 14) (1, 15)
      NAME       'b'           (1, 16) (1, 17)
      OP         ':'           (1, 17) (1, 18)
      NAME       'int'         (1, 19) (1, 22)
      OP         '='           (1, 22) (1, 23)
      NUMBER     '3'           (1, 23) (1, 24)
      OP         ')'           (1, 24) (1, 25)
      OP         '->'          (1, 26) (1, 28)
      NAME       'int'         (1, 29) (1, 32)
      OP         ':'           (1, 32) (1, 33)
      NAME       'pass'        (1, 34) (1, 38)
      """)
  
      def test_comparison(self):
          # Comparison
          self.check_tokenize("if 1 < 1 > 1 == 1 >= 5 <= 0x15 <= 0x12 != "
                              "1 and 5 in 1 not in 1 is 1 or 5 is not 1: pass", """\
      NAME       'if'          (1, 0) (1, 2)
      NUMBER     '1'           (1, 3) (1, 4)
      OP         '<'           (1, 5) (1, 6)
      NUMBER     '1'           (1, 7) (1, 8)
      OP         '>'           (1, 9) (1, 10)
      NUMBER     '1'           (1, 11) (1, 12)
      OP         '=='          (1, 13) (1, 15)
      NUMBER     '1'           (1, 16) (1, 17)
      OP         '>='          (1, 18) (1, 20)
      NUMBER     '5'           (1, 21) (1, 22)
      OP         '<='          (1, 23) (1, 25)
      NUMBER     '0x15'        (1, 26) (1, 30)
      OP         '<='          (1, 31) (1, 33)
      NUMBER     '0x12'        (1, 34) (1, 38)
      OP         '!='          (1, 39) (1, 41)
      NUMBER     '1'           (1, 42) (1, 43)
      NAME       'and'         (1, 44) (1, 47)
      NUMBER     '5'           (1, 48) (1, 49)
      NAME       'in'          (1, 50) (1, 52)
      NUMBER     '1'           (1, 53) (1, 54)
      NAME       'not'         (1, 55) (1, 58)
      NAME       'in'          (1, 59) (1, 61)
      NUMBER     '1'           (1, 62) (1, 63)
      NAME       'is'          (1, 64) (1, 66)
      NUMBER     '1'           (1, 67) (1, 68)
      NAME       'or'          (1, 69) (1, 71)
      NUMBER     '5'           (1, 72) (1, 73)
      NAME       'is'          (1, 74) (1, 76)
      NAME       'not'         (1, 77) (1, 80)
      NUMBER     '1'           (1, 81) (1, 82)
      OP         ':'           (1, 82) (1, 83)
      NAME       'pass'        (1, 84) (1, 88)
      """)
  
      def test_shift(self):
          # Shift
          self.check_tokenize("x = 1 << 1 >> 5", """\
      NAME       'x'           (1, 0) (1, 1)
      OP         '='           (1, 2) (1, 3)
      NUMBER     '1'           (1, 4) (1, 5)
      OP         '<<'          (1, 6) (1, 8)
      NUMBER     '1'           (1, 9) (1, 10)
      OP         '>>'          (1, 11) (1, 13)
      NUMBER     '5'           (1, 14) (1, 15)
      """)
  
      def test_additive(self):
          # Additive
          self.check_tokenize("x = 1 - y + 15 - 1 + 0x124 + z + a[5]", """\
      NAME       'x'           (1, 0) (1, 1)
      OP         '='           (1, 2) (1, 3)
      NUMBER     '1'           (1, 4) (1, 5)
      OP         '-'           (1, 6) (1, 7)
      NAME       'y'           (1, 8) (1, 9)
      OP         '+'           (1, 10) (1, 11)
      NUMBER     '15'          (1, 12) (1, 14)
      OP         '-'           (1, 15) (1, 16)
      NUMBER     '1'           (1, 17) (1, 18)
      OP         '+'           (1, 19) (1, 20)
      NUMBER     '0x124'       (1, 21) (1, 26)
      OP         '+'           (1, 27) (1, 28)
      NAME       'z'           (1, 29) (1, 30)
      OP         '+'           (1, 31) (1, 32)
      NAME       'a'           (1, 33) (1, 34)
      OP         '['           (1, 34) (1, 35)
      NUMBER     '5'           (1, 35) (1, 36)
      OP         ']'           (1, 36) (1, 37)
      """)
  
      def test_multiplicative(self):
          # Multiplicative
          self.check_tokenize("x = 1//1*1/5*12%0x12@42", """\
      NAME       'x'           (1, 0) (1, 1)
      OP         '='           (1, 2) (1, 3)
      NUMBER     '1'           (1, 4) (1, 5)
      OP         '//'          (1, 5) (1, 7)
      NUMBER     '1'           (1, 7) (1, 8)
      OP         '*'           (1, 8) (1, 9)
      NUMBER     '1'           (1, 9) (1, 10)
      OP         '/'           (1, 10) (1, 11)
      NUMBER     '5'           (1, 11) (1, 12)
      OP         '*'           (1, 12) (1, 13)
      NUMBER     '12'          (1, 13) (1, 15)
      OP         '%'           (1, 15) (1, 16)
      NUMBER     '0x12'        (1, 16) (1, 20)
      OP         '@'           (1, 20) (1, 21)
      NUMBER     '42'          (1, 21) (1, 23)
      """)
  
      def test_unary(self):
          # Unary
          self.check_tokenize("~1 ^ 1 & 1 |1 ^ -1", """\
      OP         '~'           (1, 0) (1, 1)
      NUMBER     '1'           (1, 1) (1, 2)
      OP         '^'           (1, 3) (1, 4)
      NUMBER     '1'           (1, 5) (1, 6)
      OP         '&'           (1, 7) (1, 8)
      NUMBER     '1'           (1, 9) (1, 10)
      OP         '|'           (1, 11) (1, 12)
      NUMBER     '1'           (1, 12) (1, 13)
      OP         '^'           (1, 14) (1, 15)
      OP         '-'           (1, 16) (1, 17)
      NUMBER     '1'           (1, 17) (1, 18)
      """)
          self.check_tokenize("-1*1/1+1*1//1 - ---1**1", """\
      OP         '-'           (1, 0) (1, 1)
      NUMBER     '1'           (1, 1) (1, 2)
      OP         '*'           (1, 2) (1, 3)
      NUMBER     '1'           (1, 3) (1, 4)
      OP         '/'           (1, 4) (1, 5)
      NUMBER     '1'           (1, 5) (1, 6)
      OP         '+'           (1, 6) (1, 7)
      NUMBER     '1'           (1, 7) (1, 8)
      OP         '*'           (1, 8) (1, 9)
      NUMBER     '1'           (1, 9) (1, 10)
      OP         '//'          (1, 10) (1, 12)
      NUMBER     '1'           (1, 12) (1, 13)
      OP         '-'           (1, 14) (1, 15)
      OP         '-'           (1, 16) (1, 17)
      OP         '-'           (1, 17) (1, 18)
      OP         '-'           (1, 18) (1, 19)
      NUMBER     '1'           (1, 19) (1, 20)
      OP         '**'          (1, 20) (1, 22)
      NUMBER     '1'           (1, 22) (1, 23)
      """)
  
      def test_selector(self):
          # Selector
          self.check_tokenize("import sys, time\nx = sys.modules['time'].time()", """\
      NAME       'import'      (1, 0) (1, 6)
      NAME       'sys'         (1, 7) (1, 10)
      OP         ','           (1, 10) (1, 11)
      NAME       'time'        (1, 12) (1, 16)
      NEWLINE    '\\n'          (1, 16) (1, 17)
      NAME       'x'           (2, 0) (2, 1)
      OP         '='           (2, 2) (2, 3)
      NAME       'sys'         (2, 4) (2, 7)
      OP         '.'           (2, 7) (2, 8)
      NAME       'modules'     (2, 8) (2, 15)
      OP         '['           (2, 15) (2, 16)
      STRING     "'time'"      (2, 16) (2, 22)
      OP         ']'           (2, 22) (2, 23)
      OP         '.'           (2, 23) (2, 24)
      NAME       'time'        (2, 24) (2, 28)
      OP         '('           (2, 28) (2, 29)
      OP         ')'           (2, 29) (2, 30)
      """)
  
      def test_method(self):
          # Methods
          self.check_tokenize("@staticmethod\ndef foo(x,y): pass", """\
      OP         '@'           (1, 0) (1, 1)
      NAME       'staticmethod' (1, 1) (1, 13)
      NEWLINE    '\\n'          (1, 13) (1, 14)
      NAME       'def'         (2, 0) (2, 3)
      NAME       'foo'         (2, 4) (2, 7)
      OP         '('           (2, 7) (2, 8)
      NAME       'x'           (2, 8) (2, 9)
      OP         ','           (2, 9) (2, 10)
      NAME       'y'           (2, 10) (2, 11)
      OP         ')'           (2, 11) (2, 12)
      OP         ':'           (2, 12) (2, 13)
      NAME       'pass'        (2, 14) (2, 18)
      """)
  
      def test_tabs(self):
          # Evil tabs
          self.check_tokenize("def f():\n"
                              "\tif x\n"
                              "        \tpass", """\
      NAME       'def'         (1, 0) (1, 3)
      NAME       'f'           (1, 4) (1, 5)
      OP         '('           (1, 5) (1, 6)
      OP         ')'           (1, 6) (1, 7)
      OP         ':'           (1, 7) (1, 8)
      NEWLINE    '\\n'          (1, 8) (1, 9)
      INDENT     '\\t'          (2, 0) (2, 1)
      NAME       'if'          (2, 1) (2, 3)
      NAME       'x'           (2, 4) (2, 5)
      NEWLINE    '\\n'          (2, 5) (2, 6)
      INDENT     '        \\t'  (3, 0) (3, 9)
      NAME       'pass'        (3, 9) (3, 13)
      DEDENT     ''            (4, 0) (4, 0)
      DEDENT     ''            (4, 0) (4, 0)
      """)
  
      def test_non_ascii_identifiers(self):
          # Non-ascii identifiers
